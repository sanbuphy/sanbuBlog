---
title: Extrinsic Hallucinations in LLMs by lilian
---
æœ¬æ–‡ä¸º Lilian æ–‡ç« çš„é˜…è¯»ç¬”è®°ã€‚Lilian åœ¨ openai å·¥ä½œï¼Œå¥¹çš„åšå®¢éƒ½éå¸¸æ¨èé˜…è¯»ï¼š[https://lilianweng.github.io/](https://lilianweng.github.io/)

è§‚åæ„Ÿï¼š

- å¯ä»¥çœ‹å‡ºå¾ˆéš¾é¿å…æœ‰å¹»è§‰ã€‚ã€‚å°¤å…¶æ˜¯ sft ä¸é è°±çš„å¯èƒ½æ€§å¾ˆå¤§é™¤éæ•°æ®å¾ˆå¥½ï¼Œæœ€å®¹æ˜“çš„è¿˜æ˜¯ RAGã€‚
- è§£å†³å¹»è§‰æœ€å¥½çš„æ–¹æ³•å»ºè®®ä» RAG çš„chunkåˆ¶ä½œå…¥æ‰‹
- é—®é—®é¢˜çš„æ–¹å¼è¦å…·ä½“ï¼Œå¯ä»¥è®©æ¨¡å‹å¤šæ£€æŸ¥ç®€å•çš„äº‹å®
- åˆ©ç”¨å‰é¢æåˆ°çš„ä¸€äº›æ–¹æ³•å¯ä»¥ç”¨æ¥ä½œä¸ºå¹»è§‰çš„ benchmarkæ£€æµ‹

## **What Causes Hallucinations?**

- out-of-date, missing, or incorrect information in pre-training data
- Introducing new knowledge at the fine-tuning stage (making it debatable whether the model can reliably learn new knowledge via small-scale fine-tuning)
  - [Gekhman et al. 2024](https://arxiv.org/abs/2405.05904)Â studied the research question of whether fine-tuning LLMs on new knowledge encourages hallucinations.
    - LLMs learn fine-tuning examples with **new knowledgeÂ *slower*Â than** other examples with knowledge consistent with the pre-existing knowledge of the model
    - the examples with new knowledge i**ncrease the modelâ€™s tendency to hallucinate**.
    - å¦‚ä½•æ ¹æ®æ¨¡å‹åœ¨é—­å·é—®ç­”æ•°æ®é›†ï¼ˆå¦‚EntityQuestionsï¼‰ä¸Šç”Ÿæˆæ­£ç¡®ç­”æ¡ˆçš„å¯èƒ½æ€§ï¼Œå¯¹æ¨¡å‹çš„çŸ¥è¯†è¿›è¡Œåˆ†ç±»ã€‚ç»™å®šä¸€ä¸ªé—®é¢˜ğ‘å’Œç­”æ¡ˆğ‘ï¼Œå®šä¹‰ğ‘ƒCorrect(ğ‘,ğ‘;ğ‘€,ğ‘‡)æ¥ä¼°è®¡æ¨¡å‹ğ‘€åœ¨æ¸©åº¦ğ‘‡ä¸‹ç”Ÿæˆæ­£ç¡®ç­”æ¡ˆğ‘çš„å¯èƒ½æ€§ã€‚

     | Type | Category | Definition | Explanation |
     | --- | --- | --- | --- |
     | Known | HighlyKnown | $$ P_{\text{Correct}}(q, a; M, T = 0) = 1 $$ | Greedy decoding always predicts the correct answer. |
     |  | MaybeKnown | $$ P_{\text{Correct}}(q, a; M, T = 0) \in (0, 1) $$ | Greedy decoding sometimes (but not always) predicts the correct answer. |
     |  | WeaklyKnown | $$ P_{\text{Correct}}(q, a; M, T = 0) = 0 $$ âˆ§ $$ P_{\text{Correct}}(q, a; M, T > 0) > 0 $$ | Greedy decoding never predicts the correct answer, whereas temperature sampling with $$ T > 0 $$ sometimes predicts the correct answer. |
     | Unknown | Unknown | $$ P_{\text{Correct}}(q, a; M, T \geq 0) = 0 $$ | The model never predicts the correct answer, thus it seems to lack the knowledge of the correct answer. |

The best dev performance is obtained when the LLM fits **the majority of theÂ `Known`Â training examples but only a few of theÂ `Unknown`Â ones**. The model starts to hallucinate when it learns most of theÂ `Unknown`Â examples.

![Untitled](Extrinsic-Hallucinations-in-LLMs-by-lilian-ae7d4159ecd04d8caad2e9d08d06e941/Untitled.png)

## **Hallucination Detection**

### **Retrieval-Augmented Evaluation**

[Lee et al. (2022)](https://arxiv.org/abs/2206.04624)Â introduced a new benchmark dataset,Â **FactualityPrompt**,consisting of both factual and nonfactual prompts.

This dataset uses Wikipedia documents or sentences as the knowledge base for factuality grounding.The Wikipedia documents are known ground-truth from theÂ [FEVER](https://fever.ai/dataset/fever.html)Â dataset,

![Untitled](Extrinsic-Hallucinations-in-LLMs-by-lilian-ae7d4159ecd04d8caad2e9d08d06e941/Untitled%201.png)

1. **Hallucination NE (Named Entity) errors**: ä½¿ç”¨é¢„è®­ç»ƒçš„å®ä½“æ£€æµ‹æ¨¡å‹å’Œæ–‡æ¡£çº§groundingçŸ¥è¯†ï¼Œè¯¥æŒ‡æ ‡æµ‹é‡æœªå‡ºç°åœ¨groundingæ–‡æ¡£ä¸­çš„æ£€æµ‹åˆ°çš„å‘½åå®ä½“çš„æ¯”ä¾‹ã€‚
2. **Entailment ratios**: ä½¿ç”¨åŸºäºMNLIå’Œå¥å­çº§çŸ¥è¯†åŸºç¡€å¾®è°ƒçš„RoBERTaæ¨¡å‹ï¼Œè¯¥æŒ‡æ ‡è®¡ç®—ç”Ÿæˆå¥å­çš„åˆ†æ•°ï¼Œè¿™äº›å¥å­è¢«è•´æ¶µæ¨¡å‹æ ‡è®°ä¸ºä¸é…å¯¹çš„Wikipediaå¥å­ç›¸å…³

```notion
æ–‡æœ¬è•´æ¶µ(Textual entailment)
entailment:
the relationship between two statements when for one to be true, 
the other must also be true

åœ¨è‡ªç„¶è¯­è¨€å¤„ç†(natural language processing)ä¸­ï¼Œæ–‡æœ¬è•´æ¶µ(Textual entailment)æ˜¯æŒ‡ä¸¤ä¸ªæ–‡æœ¬ç‰‡æ®µæœ‰æŒ‡å‘å…³ç³»ã€‚å½“è®¤ä¸ºä¸€ä¸ªæ–‡æœ¬ç‰‡æ®µçœŸå®æ—¶ï¼Œå¯ä»¥æ¨æ–­å‡ºå¦ä¸€ä¸ªæ–‡æœ¬ç‰‡æ–­çš„çœŸå®æ€§ã€‚ä¹Ÿå°±æ˜¯æŒ‡ï¼Œä¸€ä¸ªæ–‡æœ¬ç‰‡æ®µè•´æ¶µäº†å¦ä¸€ä¸ªæ–‡æœ¬ç‰‡æ®µçš„çŸ¥è¯†ã€‚å¯ä»¥åˆ†åˆ«ç§°è•´æ¶µçš„æ–‡æœ¬(entailing texts)ä¸ºæ–‡æœ¬(text)ï¼Œè¢«è•´æ¶µçš„æ–‡æœ¬(entailed texts)ä¸ºå‡è®¾(hypothesis)ã€‚

ä¸€ä¸ªæ­£é¢çš„æ–‡æœ¬è•´æ¶µçš„ä¾‹å­(æ–‡æœ¬è•´æ¶µäº†å‡è®¾):

     æ–‡æœ¬ï¼šå¦‚æœä½ å¸®åŠ©éœ€è¦çš„äººï¼Œä¸Šå¸å°±ä¼šæŠ¥ç­”ä½ ã€‚

     å‡è®¾ï¼šæ‹¿é’±ç»™ç©·äººå¯ä»¥å¾—åˆ°å¥½çš„ç»“æœã€‚

ä¸€ä¸ªè´Ÿé¢çš„æ–‡æœ¬è•´æ¶µçš„ä¾‹å­(æ–‡æœ¬æ¨ç¿»äº†å‡è®¾):

     æ–‡æœ¬ï¼šå¦‚æœä½ å¸®åŠ©éœ€è¦çš„äººï¼Œä¸Šå¸å°±ä¼šæŠ¥ç­”ä½ ã€‚

     å‡è®¾ï¼šæ‹¿é’±ç»™ç©·äººä¸ä¼šæœ‰ç»“æœã€‚

ä¸€ä¸ªä¸æ˜¯æ–‡æœ¬è•´æ¶µçš„ä¾‹å­(æ–‡æœ¬å³ä¸èƒ½è•´æ¶µå‡è®¾ä¹Ÿä¸èƒ½æ¨ç¿»å‡è®¾):

     æ–‡æœ¬ï¼šå¦‚æœä½ å¸®åŠ©éœ€è¦çš„äººï¼Œä¸Šå¸å°±ä¼šæŠ¥ç­”ä½ ã€‚

     å‡è®¾ï¼šæ‹¿é’±ç»™ç©·äººä¼šè®©ä½ æˆä¸ºæ›´å¥½çš„äººã€‚

ä»ä¸­å¯ä»¥çœ‹å‡ºï¼Œæ–‡æœ¬è•´æ¶µå…³ç³»ä¸æ˜¯çº¯ç²¹çš„é€»è¾‘æ¨ç†ï¼Œå®ƒçš„æ¡ä»¶æ›´ä¸ºå®½æ¾ï¼Œå¯ä»¥è¿™æ ·å®šä¹‰ï¼šå¦‚æœä¸€ä¸ªäººè¯»äº†tèƒ½å¤Ÿæ¨è®ºhéå¸¸å¯èƒ½æ˜¯çœŸå®çš„ï¼Œé‚£ä¹ˆt è•´æ¶µ h(t=>h)ã€‚
```

1. **FActScore**ï¼ˆåŸå­æ€§åˆ†æ•°ä¸­çš„äº‹å®ç²¾åº¦ï¼›[Minet al.2023](https://arxiv.org/abs/2305.14251)ï¼‰å°†é•¿å½¢å¼ç”Ÿæˆåˆ†è§£ä¸ºå¤šä¸ªåŸå­äº‹å®ï¼Œå¹¶æ ¹æ®ç»´åŸºç™¾ç§‘ç­‰çŸ¥è¯†åº“åˆ†åˆ«éªŒè¯æ¯ä¸ªåŸå­äº‹å®ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥æµ‹é‡æ¯ä¸ªæ¨¡å‹ç”Ÿæˆçš„çŸ¥è¯†æºæ”¯æŒçš„å¥å­çš„æ¯”ç‡ï¼ˆç²¾åº¦ï¼‰ï¼ŒFActScoreæ˜¯ä¸€ç»„æç¤ºä¸­æ¨¡å‹ç”Ÿæˆçš„å¹³å‡ç²¾åº¦

å…³äºæ¨¡å‹å¹»è§‰è¡Œä¸ºçš„ä¸€äº›æœ‰è¶£è§‚å¯Ÿï¼š

- ä¼ è®°ç”Ÿæˆä»»åŠ¡ä¸­è¾ƒç½•è§çš„å®ä½“çš„é”™è¯¯ç‡è¾ƒé«˜ã€‚
- å¯¹äºç”Ÿæˆä¸­é åæåˆ°çš„äº‹å®ï¼Œé”™è¯¯ç‡æ›´é«˜ã€‚
- ä½¿ç”¨æ£€ç´¢æ¥å»ºç«‹æ¨¡å‹ç”Ÿæˆæ˜¾ç€æœ‰åŠ©äºå‡å°‘å¹»è§‰ã€‚

### **Sampling-Based Detection**

- **SelfCheckGPT**ï¼ˆ[Manakulç­‰äººã€‚2023](https://arxiv.org/abs/2303.08896)ï¼‰ç®€å•æ¥è¯´å°±æ˜¯é‡‡æ ·å¤šä¸ªæ ·æœ¬ï¼Œç„¶åæ‹¿llmåˆ¤æ–­æ˜¯å¦æ­£ç¡®ã€‚

ä¾èµ–äºå¯¹æ¥è‡ªé»‘ç›’LLMçš„å¤šä¸ªæ ·æœ¬çš„äº‹å®é”™è¯¯çš„ä¸€è‡´æ€§æ£€æŸ¥ã€‚è€ƒè™‘åˆ°ç°ç›’äº‹å®æ£€æŸ¥æµ‹é‡éœ€è¦è®¿é—®LLMçš„ä»¤ç‰Œçº§logprobï¼ŒSelfCheckGPTåªéœ€è¦ä¸ä¾èµ–å¤–éƒ¨çŸ¥è¯†åº“çš„æ ·æœ¬ï¼Œå› æ­¤é»‘ç›’è®¿é—®å°±è¶³å¤Ÿäº†ï¼Œä¸éœ€è¦å¤–éƒ¨çŸ¥è¯†åº“ã€‚

![Untitled](Extrinsic-Hallucinations-in-LLMs-by-lilian-ae7d4159ecd04d8caad2e9d08d06e941/Untitled%202.png)

### **Calibration of Unknown Knowledge å¯¹äºæœªçŸ¥çš„å¹»è§‰**

- æç¤ºæ¨¡å‹å¯¹æ— æ³•å›ç­”æˆ–æœªçŸ¥çš„é—®é¢˜äº§ç”Ÿååº”å¯èƒ½ä¼šå¼•å‘å¹»è§‰ã€‚TruthfulQAï¼ˆ[Lin et al.2021](https://arxiv.org/abs/2109.07958)ï¼‰å’ŒSelfAwareï¼ˆ[å°¹et al.2023](https://arxiv.org/abs/2305.18153)ï¼‰æ˜¯è¡¡é‡æ¨¡å‹åœ¨è¿™ç§æƒ…å†µä¸‹èƒ½åœ¨å¤šå¤§ç¨‹åº¦ä¸Šäº§ç”ŸçœŸå®ååº”çš„ä¸¤ä¸ªåŸºå‡†
- åœ¨[TruthfulQA](https://github.com/sylinrl/TruthfulQA)ï¼ˆ[Lin et al.2021](https://arxiv.org/abs/2109.07958)ï¼‰ä¸­çš„æµ‹è¯•é—®é¢˜æ˜¯æ ¹æ®äººç±»å¸¸è§çš„è¯¯è§£æˆ–é”™è¯¯*å¯¹æŠ—æ€§åœ°*åˆ¶ä½œçš„ã€‚è¯¥åŸºå‡†åŒ…æ‹¬817ä¸ªé—®é¢˜ï¼Œæ¶µç›–38ä¸ªä¸»é¢˜ï¼ŒåŒ…æ‹¬å¥åº·ã€æ³•å¾‹ã€é‡‘èå’Œæ”¿æ²»ã€‚**æœ€å¥½çš„LLMåœ¨æ¯”è¾ƒä¸­çš„å‡†ç¡®ç‡ä¸º58%ï¼Œäººç±»å¯ä»¥è¾¾åˆ°94%ã€‚**
- [å°¹ç­‰ï¼ˆ2023ï¼‰](https://arxiv.org/abs/2305.18153)ç ”ç©¶*è‡ªæˆ‘è®¤è¯†çš„*æ¦‚å¿µï¼ŒæŒ‡çš„æ˜¯è¯­è¨€æ¨¡å‹æ˜¯å¦çŸ¥é“è‡ªå·±çŸ¥é“ä»€ä¹ˆæˆ–ä¸çŸ¥é“ä»€ä¹ˆã€‚Â **SelfAware**ï¼ŒåŒ…å«äº”ä¸ªç±»åˆ«çš„1032ä¸ªæ— æ³•å›ç­”çš„é—®é¢˜å’Œ2337ä¸ªå¯å›ç­”çš„é—®é¢˜ã€‚ï¼ˆä»ç»“æœæ¥çœ‹åŸºæœ¬ä¸Šå¤§æ¨¡å‹gpt4æœ€å¥½éƒ½åªèƒ½42%ï¼Œå…¶ä»–éƒ½æ˜¯æ›´å°‘ã€‚ï¼ˆ2023å¹´ï¼‰

## **Anti-Hallucination Methods é˜»æ­¢å¹»è§‰**

### **Indirect Query é€šè¿‡é—´æ¥æŸ¥è¯¢åˆ¤æ–­**

*ç›´æ¥æŸ¥è¯¢*è¦æ±‚æ¨¡å‹åˆ¤æ–­ç”Ÿæˆçš„å¼•ç”¨æ˜¯å¦å­˜åœ¨ã€‚**é—´æ¥æŸ¥è¯¢**æ”¹ä¸ºè¯¢é—®ç”Ÿæˆçš„å¼•ç”¨çš„è¾…åŠ©ç»†èŠ‚â€”â€”ä½œè€…æ˜¯è°ï¼›ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬æƒ³æ£€æŸ¥`"Is the following paper real?"`ï¼Œæˆ‘ä»¬å¯ä»¥æ£€æŸ¥`"Who are the author of the paper?"`

![Untitled](Extrinsic-Hallucinations-in-LLMs-by-lilian-ae7d4159ecd04d8caad2e9d08d06e941/Untitled%203.png)

### RAG

[RAG (Retrieval-augmented Generation)](https://lilianweng.github.io/posts/2020-10-29-odqa/#RAG)Â is a very common approach to provide åŸºç¡€ä¿¡æ¯  grounding information, that is to retrieve relevant documents and then generate with related documents as extra context.

- **RARR**Â (â€œRetrofit Attribution using Research and Revisionâ€;Â [Gao et al. 2022](https://arxiv.org/abs/2210.08726))
- ä¸ä½¿ç”¨æœç´¢+ç¼–è¾‘çš„RARRç±»ä¼¼ï¼Œ**FAVA**ï¼ˆâ€œä½¿ç”¨å¢å¼ºçŸ¥è¯†è¿›è¡Œäº‹å®éªŒè¯â€ï¼›Mishraç­‰äººã€‚2024ï¼‰ä¹Ÿæ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼Œç„¶åç¼–è¾‘æ¨¡å‹è¾“å‡ºä»¥é¿å…å¹»è§‰é”™è¯¯ã€‚
- é‡æ–°æ€è€ƒæ£€ç´¢ï¼ˆRRï¼›He et al.2022ï¼‰æ–¹æ³•ä¹Ÿä¾èµ–äºæ£€ç´¢ç›¸å…³çš„å¤–éƒ¨çŸ¥è¯†ï¼Œä½†ä¸éœ€è¦é¢å¤–çš„ç¼–è¾‘ã€‚RRçš„æ£€ç´¢ä¸æ˜¯åˆ©ç”¨æœç´¢æŸ¥è¯¢ç”Ÿæˆæ¨¡å‹ï¼Œè€Œæ˜¯åŸºäºåˆ†è§£çš„CoTæç¤º
- Self-RAGï¼ˆâ€œè‡ªæˆ‘åæ€æ£€ç´¢-å¢å¼ºç”Ÿæˆâ€ï¼›Asai et al.2024ï¼‰é€šè¿‡è¾“å‡ºä»»åŠ¡è¾“å‡ºå’Œé—´æ­‡æ€§ç‰¹æ®Šåå°„ä»¤ç‰Œæ¥è®­ç»ƒLLMç«¯åˆ°ç«¯å­¦ä¹ åæ€è‡ªå·±çš„ç”Ÿæˆã€‚ä»–ä»¬é€šè¿‡æç¤ºGPT-4ä¸ºæ‰¹è¯„å®¶æ¨¡å‹å’Œç”Ÿæˆå™¨æ¨¡å‹åˆ›å»ºäº†ç›‘ç£æ•°æ®é›†ï¼Œç„¶åå°†å…¶æç‚¼æˆå†…éƒ¨æ¨¡å‹ä»¥é™ä½æ¨ç†æˆæœ¬ã€‚

### **Chain of Actions**

Without grounding by external retrieved knowledge, we can design a process for using the model itself to do verification and revision to reduce hallucination.

- Dhuliawala ç­‰äººï¼ˆ2023ï¼‰æå‡ºäº†ä¸€ç§åä¸ºâ€œéªŒè¯é“¾â€ï¼ˆChain-of-Verification, CoVeï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡ä¸€ç³»åˆ—çš„è¡ŒåŠ¨æ¥è§„åˆ’å’Œæ‰§è¡ŒéªŒè¯ã€‚CoVe åŒ…å«å››ä¸ªæ ¸å¿ƒæ­¥éª¤ï¼šé¦–å…ˆï¼Œæ¨¡å‹ç”Ÿæˆä¸€ä¸ªåˆå§‹è‰ç¨¿ï¼Œç§°ä¸ºâ€œåŸºçº¿â€ã€‚æ¥ç€ï¼Œæ¨¡å‹åŸºäºè¿™æ¬¡ç”Ÿæˆè®¾è®¡éæ¨¡æ¿åŒ–çš„éªŒè¯é—®é¢˜è¿›è¡Œäº‹å®æ£€æŸ¥ï¼Œè¿™å¯ä»¥é€šè¿‡å°‘é‡ç¤ºä¾‹æç¤ºï¼ˆresponse, verification questionsï¼‰å®ç°ã€‚ç¬¬ä¸‰æ­¥æ˜¯ç‹¬ç«‹å›ç­”è¿™äº›é—®é¢˜ï¼Œæœ‰å‡ ç§å˜ä½“è®¾ç½®ï¼š(1) è”åˆï¼šä¸ç¬¬äºŒæ­¥ç»“åˆï¼Œå°‘é‡ç¤ºä¾‹ç»“æ„ä¸ºï¼ˆresponse, verification questions, verification answersï¼‰ï¼›ç¼ºç‚¹æ˜¯åŸå§‹å“åº”åœ¨ä¸Šä¸‹æ–‡ä¸­ï¼Œæ¨¡å‹å¯èƒ½ä¼šé‡å¤ç±»ä¼¼çš„å¹»è§‰ã€‚(2) ä¸¤æ­¥ï¼šå°†éªŒè¯è§„åˆ’å’Œæ‰§è¡Œæ­¥éª¤åˆ†å¼€ï¼Œé¿å…åŸå§‹å“åº”çš„å½±å“ã€‚(3) åˆ†è§£ï¼šæ¯ä¸ªéªŒè¯é—®é¢˜å•ç‹¬å›ç­”ã€‚å¦‚æœé•¿æ–‡æœ¬ç”Ÿæˆå¯¼è‡´å¤šä¸ªéªŒè¯é—®é¢˜ï¼Œæˆ‘ä»¬ä¼šé€ä¸€å›ç­”æ¯ä¸ªé—®é¢˜ã€‚(4) åˆ†è§£+ä¿®è®¢ï¼šåœ¨åˆ†è§£éªŒè¯æ‰§è¡Œåå¢åŠ â€œäº¤å‰æ£€æŸ¥â€æ­¥éª¤ï¼ŒåŸºäºåŸºçº¿å“åº”å’ŒéªŒè¯é—®é¢˜åŠç­”æ¡ˆæ¥æ£€æµ‹ä¸ä¸€è‡´ã€‚æœ€åä¸€æ­¥æ˜¯ç”Ÿæˆæœ€ç»ˆã€æ”¹è¿›åçš„è¾“å‡ºï¼Œå¦‚æœå‘ç°ä¸ä¸€è‡´åˆ™è¿›è¡Œä¿®è®¢ã€‚

    ![Untitled](Extrinsic-Hallucinations-in-LLMs-by-lilian-ae7d4159ecd04d8caad2e9d08d06e941/Untitled%204.png)

    Here are some interesting observations from the CoVe experiments:

  - Instruction-tuning andÂ [CoT](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/#chain-of-thought-cot)Â do not reduce hallucinations.
  - Factored and 2-step CoVe improve performance and further explicit reasoning on inconsistency detection also helps (â€œfactor+reviseâ€ approach).
  - Short-form verification questions are more accurately answered than long-form queries.
  - Free-form LLM-generated verification questions are better than heuristics (e.g.Â `Does X answer the question?`) and questions that require open-ended generation work better than yes/no questions.

- **RECITE**Â (â€œRecitation-augmented generationâ€;Â [Sun et al. 2023](https://arxiv.org/abs/2210.01296)) relies on recitation as an intermediate step to improve factual correctness of model generation and reduce hallucination.

### Sampling Methods

[Lee, et al. (2022)](https://arxiv.org/abs/2206.04624) found that[nucleus sampling](https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/#nucleus)(top-ğ‘sampling) is found to perform worse on [FactualityPrompt](https://github.com/nayeon7lee/FactualityPrompt) benchmark than greedy sampling, although it achieves better diversity and less repetition, since nucleus sampling added extra randomness.

### **Fine-tuning for Factuality**

[Lee, et al. (2022)](https://arxiv.org/abs/2206.04624)Â proposed two ideas for factuality-enhanced training:

- `TopicPrefix`  Append topic (i.e. wikipedia document title) in front of each sentence in this document.
- æ›´æ–°è®­ç»ƒæŸå¤±ä»¥å…³æ³¨å¥å­çš„ååŠéƒ¨åˆ†ï¼Œä»–ä»¬å‡è®¾å¥å­çš„ååŠéƒ¨åˆ†åŒ…å«æ›´å¤šçš„äº‹å®çŸ¥è¯†ã€‚ The implementation is quite simple, deciding a pivotÂ , and all the tokens before theÂ th token are all applied zero-masking. In their experiment, the best pivotÂ Â is selected as 0.5 x the sentence length.

## Appendix: Evaluation Benchmarks

Here is a list of datasets mentioned in this post.

[**TruthfulQA**](https://github.com/sylinrl/TruthfulQA)Â ([Lin et al. 2021](https://arxiv.org/abs/2109.07958)) is designed to measure how well a LLM can generate truthful responses. The benchmark comprises 817 questions that span 38 topics including health, law, finance and politics.

[**FactualityPrompt**](https://github.com/nayeon7lee/FactualityPrompt)Â ([Lee, et al. 2022](https://arxiv.org/abs/2206.04624)) is a benchmark consisting of both factual and nonfactual prompts. It relies on Wikipedia documents or sentences as the knowledge base for factuality grounding.

[**SelfAware**](https://github.com/yinzhangyue/SelfAware)Â ([Yin et al. 2023](https://arxiv.org/abs/2305.18153)) contains 1,032 unanswerable questions across five categories and 2,337 answerable questions. Unanswerable questions are sourced from online forums with human annotations while answerable questions are sourced from SQuAD, HotpotQA and TriviaQA based on text similarity with unanswerable questions.

[**LongFact**](https://github.com/google-deepmind/long-form-factuality/tree/main/longfact)Â ([Wei et al. 2024](https://arxiv.org/abs/2403.18802)Â ) is designed for checking long-form generation factuality. It consists of 2280 fact-seeking prompts that seek long-form responses on 38 manually curated topics

[**HaDes**](https://github.com/microsoft/HaDes)Â ([Liu et al. 2021](https://arxiv.org/abs/2104.08704)) is a benchmark for hallucination detection as a binary classification task. The dataset is created by perturbing Wikipedia text and human annotation.

[**FEVER**](https://fever.ai/dataset/fever.html)Â (Fact Extraction and VERification) dataset contains 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. Each claim is classified asÂ `Supported`,Â `Refuted`Â orÂ `NotEnoughInfo`.

[**FAVABench**](https://huggingface.co/datasets/fava-uw/fava-data)Â ([Mishra et al. 2024](https://arxiv.org/abs/2401.06855)) is a benchmark for evaluating fine-grained hallucination. There are 200 information-seeking source prompts and 3 model responses per prompt, resulting in 600 responses in total. Each model response is manually labeled with fine-grained annotations on hallucination error types.
