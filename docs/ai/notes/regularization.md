---
id: L1、L2正则化理论与实践
---

源于网络上很多资料估计自己都没弄懂，不如自己总结一篇。

## 正则化理论

网络上流传着一张图：

所以，他到底是什么意思？你需要从损失函数开始理解。

### 损失函数投影

对于一个平方损失函数，容易写出他的形式：

对于一个简单的场景，比如只有两个参数 w1 w1, 整理后，他是一个二次型：

对于一个简化后的二次型：

我们可以查看他的三维表示：

对于相同的损失函数值，即假设 z = C 截取了某个二维平面，在这个平面与损失函数的交线就是等高线，表示在这个线上损失函数值相同

同理，我们可以画出多个等高线，同时梯度下降的本质就是尽量朝着圆心出发

### L1、L2区别

### 为什么正则化能限制范围

此时我们已经有了损失函数在二维平面上的基础表示，我们也知道 loss 下降实质是外围等高线向内部出发。那对于 L1、L2 又是如何对 loss 的下降过程进行约束？

### 权重变化

如果认为少数参数核心作用，可以使用 L1,多数参数有效，使用 L2，为何？

## Pytorch中的正则化

//TODO 这个部分存在问题。。
在pytorch中，我们可以通过在 loss 函数后加上一个正则化项来施加惩罚（意思就是让你的loss变小，所以权重的变化就变小，学习的速度）

## 与权重衰减的区别

## Reference
